神经网络已经成为我们日常生活中不可或缺的一部分，无论是公开的（以大型语言模型，LLM的形式），还是隐藏在背后，驱动着无数技术和科学发现，包括无人机、汽车、搜索引擎、分子设计和推荐系统[WFD+23]。正如我们所看到的，所有这些成就都是基于相同的指导原则和组件，而这些原则和组件构成了本书的核心，同时研究的重点已转向将这些原则和组件扩展到物理上可实现的极限。

扩展的力量体现在最近提出的神经网络扩展法则（neural scaling laws）这一概念中，后者在推动人工智能（AI）大规模投资中发挥了重要作用[KMH+20，HBE+24]：非正式地说，对于几乎任何任务，只要同时增加数据、计算能力和模型的规模总是可以提高预测的精度。


. As we will see, all
of this has been done by relying on a very small set of
guiding principles and components, forming the core of
this book, while the research focus has shifted to scaling
them up to the limits of what is physically possible.
The power of scaling is embodied in the relatively recent
concept of neural scaling laws, which in turn has been
instrumental in driving massive investments in artificial
intelligence (AI) [KMH+20, HBE+24]: informally, for
practically any task, simultaneously increasing data,
compute power, and the size of the models – almost
always – results in a predictable increase in accuracy.
Stated in another way, the compute power required to
achieve a given accuracy for a task is decreasing by a
constant factor per period of time [HBE+24]. The
tremendous power of combining simple, general-purpose
tools with exponentially increased computational power in